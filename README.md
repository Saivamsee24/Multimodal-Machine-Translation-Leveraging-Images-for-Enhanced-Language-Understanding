This project implements a multimodal machine translation (MMT) system that integrates visual information from images with textual context to improve translation quality.
By combining a vision encoder (SigLIP) with a multilingual encoderâ€“decoder (mBART-50) and applying LoRA-based fine-tuning, the model achieves consistent improvements over text-only baselines.

The project is designed for research reproducibility, modular extension, and publication-ready experimentation.
